{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster analysis of patron activity in their gaming account.\n",
    "L1D and L1W are deposits/withdrawals by the patron to/from their gaming account\n",
    "L2D and L2W are deposits/withdrawals by the patron to their gaming account to place while gaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load up the packages for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import itertools\n",
    "import datetime as dt\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up some display and format defaults for the data\n",
    "    #1. show up to 999 columns\n",
    "    #2. set the number of decimal places in floats to 2\n",
    "    #3. ignore warnings when slicing a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 999)\n",
    "pd.options.display.float_format = '{:20. 2f}'.format\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data\n",
    "\n",
    "    #The df had problems with the float format in the TransactionAmount \n",
    "    #column,there could a problem with some/all of the data\n",
    "\n",
    "#   The option to load in the data as all string fixed the issue\n",
    "\n",
    "        #df = pd.read_csv(\"Online_casino_DIB.csv\", encoding='utf-8')\n",
    "        #df = pd.read_csv(\"Online_casino_DIB.csv\", dtype='str')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Online_casino_DIB_A.csv\", dtype='str')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new df to clean up and set up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you want to handle NaN values, you can fill or drop them\n",
    "\n",
    "    #Example: filling NaNs with 0\n",
    "        #df['TransactionAmount'] = df['TransactionAmount'].fillna(0)  \n",
    "#or\n",
    "        #Example: dropping rows with NaNs in 'TransactionAmount'\n",
    "        #df.dropna(subset=['TransactionAmount'], inplace=True)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Change the column names to simpler editions\n",
    "#2. Change dtype for ReqTimeUTC to date/time and trim off the time data\n",
    "#3. Change the TransactionAmount into float\n",
    "#4. Check df for bad data entries - none found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = df.copy()\n",
    "\n",
    "df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Troubleshooting tips when a float column won't load correctly:\n",
    "\n",
    "#problematic_entries = df_raw['TransactionAmount'].apply(pd.to_numeric, \n",
    "#   errors='coerce').isnull()\n",
    "\n",
    "#if problematic_entries.any():\n",
    "    #print(\"Found problematic entries:\")\n",
    "    #print(df_raw[problematic_entries])\n",
    "\n",
    "#Convert column back to float, with coercing invalid values to NaN\n",
    "\n",
    "#df_raw['TransactionAmount'] = pd.to_numeric(df_raw['TransactionAmount'], \n",
    "#    errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert column to int since values < 1.00 aren't significant\n",
    "\n",
    "df_raw['TransactionAmount'] = df_raw['TransactionAmount'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert ReqTimeUTC column to date/time\n",
    "\n",
    "#df_raw[\"ReqTimeUTC\"] = df_raw[\"ReqTimeUTC\"].astype('datetime64[ns]')\n",
    "\n",
    "#df_raw[\"ReqTimeUTC\"] = pd.to_datetime(df_raw[\"ReqTimeUTC\"], utc=True)\n",
    "\n",
    "df_raw[\"ReqTimeUTC\"] = pd.to_datetime(df_raw[\"ReqTimeUTC\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start here by removing the time stamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change column names into more usable format\n",
    "\n",
    "df_raw.rename(columns= {'AccountIdentifier': 'user', 'Status': 'status', \n",
    "                    'TransactionType': 'type',\n",
    "                    'ReqTimeUTC': 'date', 'TransactionAmount': 'amount'},\n",
    "                    inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sweep for NaNs - we have none\n",
    "\n",
    "nan = df_raw[df_raw.isnull().all(axis=1)].index\n",
    "nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use REGEX to drop the leading 'customer' for the user column - too clumsy to \n",
    "\n",
    "df_raw['user'] = df_raw['user'].str.replace(r'^customer', '',\n",
    "                                             regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw['type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the types into a code and use the mapping in the column\n",
    "\n",
    "df_raw['type'] = df_raw['type']\\\n",
    "    .map({'LOYALTYCARDDEBIT': 'L2D', 'LOYALTYCARDCREDITCL': 'L1D',\n",
    "          'LOYALTYCARDCREDIT': 'L2W'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw['type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new df's from df_raw to use to work the data.\n",
    "\n",
    "#Use only the APPROVED L1D data for this first analysis\n",
    "\n",
    "#Good practice to reset the index after extracting data for future work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_working = df_raw.copy()\n",
    "\n",
    "#df_working.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame to get only rows where 'type' is 'L1D'\n",
    "df_l1d = df_working[df_working['type'] == 'L1D']\n",
    "df_l1d.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame to get only rows where 'status' is 'APPROVED'\n",
    "df_approved = df_l1d[df_l1d['status'] == 'APPROVED']\n",
    "df_approved.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the data on on the patron and start to create the RFM features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg = df_approved.groupby(by='user', as_index=False) \\\n",
    ".agg(\n",
    "    value = ('amount', 'sum'),\n",
    "    freq = ('amount', 'nunique'),\n",
    "    last = ('date', 'max')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_date = df_agg['last'].max()\n",
    "\n",
    "df_agg['recent'] = (max_date - pd.to_datetime(df_agg['last']))\\\n",
    "    .dt.days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Extract the date part\n",
    "#   df_agg['last_date'] = df_agg['last'].dt.date\n",
    "\n",
    "#Example max_date for the calculation\n",
    "#   max_date = pd.to_datetime('2023-12-09')\n",
    "\n",
    "#Calculate the number of days between two dates\n",
    "#   df_agg['recent'] = (max_date - pd.to_datetime\\\n",
    "#       (df_agg['last_date'])).dt.days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot raw MFR features\n",
    "\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(15, 5))\n",
    "\n",
    "sns.boxplot(data=df_agg['value'], color='skyblue', ax=ax[0])\n",
    "\n",
    "sns.boxplot(data=df_agg['freq'], color='orange', ax=ax[1])\n",
    "\n",
    "sns.boxplot(data=df_agg['recent'], color='salmon', ax=ax[2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The boxplots show that the extreme outliers in Monetary Value and Frequency.\n",
    "\n",
    "# Remove and save the extremes values for independent consideration if possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort out the outliers by removing any anything +/- 1.5*IQR\n",
    "\n",
    "M_Q1 = df_agg['value'].quantile(0.25)\n",
    "\n",
    "M_Q3 = df_agg['value'].quantile(0.75)\n",
    "\n",
    "M_IQR = M_Q3 - M_Q1\n",
    "\n",
    "M_OLH = M_Q3 + 1.5 * M_IQR\n",
    "M_OLL = M_Q1 - 1.5 * M_IQR\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_value_outliers = df_agg[(df_agg['value'] > M_OLH)\n",
    "                           | (df_agg['value'] < M_OLL)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_value_outliers.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_Q1 = df_agg['freq'].quantile(0.25)\n",
    "\n",
    "F_Q3 = df_agg['freq'].quantile(0.75)\n",
    "\n",
    "F_IQR = F_Q3 - F_Q1\n",
    "\n",
    "F_OLH = F_Q3 + 1.5 * F_IQR\n",
    "F_OLL = F_Q1 - 1.5 * F_IQR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_freq_outliers = df_agg[(df_agg['freq'] > F_OLH)\n",
    "                           | (df_agg['freq'] < F_OLL)]\\\n",
    "                            .copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_freq_outliers.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new df for data w/ outliers removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_outliers = df_agg[(~df_agg.index.isin(df_value_outliers)) &\n",
    "                        (~df_agg.index.isin(df_freq_outliers))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_outliers.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redo the boxplots on the data with the outliers removed to see \n",
    "#if there is any useful difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3, figsize=(15, 5))\n",
    "\n",
    "sns.boxplot(data=df_no_outliers['value'], color='skyblue',\n",
    "             ax=ax[0])\n",
    "\n",
    "sns.boxplot(data=df_no_outliers['freq'], color='orange',\n",
    "             ax=ax[1])\n",
    "\n",
    "sns.boxplot(data=df_no_outliers['recent'], color='salmon',\n",
    "             ax=ax[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The effect of removing he outliers is not useful overall, \n",
    "# so use the df with the most data in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-DScatterplot of the aggregated data\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "ax = fig.add_subplot(projection=\"3d\")                \n",
    "\n",
    "scatter = ax.scatter(df_agg['value'], df_agg['freq'], \n",
    "                     df_agg['recent'])\n",
    "\n",
    "ax.set_xlabel('Deposit Value')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_zlabel('Recency')\n",
    "\n",
    "ax.set_title('3-D Scatterplot of Aggregated Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 3-D Scatter plot shows the data heavily skewed to the right \n",
    "# meaning the large majority of deposits are small, not frequent and \n",
    "# strongly variable in recency.\n",
    "\n",
    "# Scale the data and replot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "scaled_data = scaler.fit_transform(df_agg[['value', 'freq',\n",
    "                                            'recent']])\n",
    "\n",
    "scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled_ss = pd.DataFrame(scaled_data, index=df_agg.index,\n",
    "                            columns= ('value', 'freq', 'recent'))\n",
    "\n",
    "df_scaled_ss.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replot the data scaled with StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "ax = fig.add_subplot(projection=\"3d\")                \n",
    "\n",
    "scatter = ax.scatter(df_scaled_ss['value'], df_scaled_ss['freq'],\n",
    "                      df_scaled_ss['recent'])\n",
    "\n",
    "ax.set_xlabel('Deposit Value')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_zlabel('Recency')\n",
    "\n",
    "ax.set_title('3-D Scatterplot of Standard Scaled Aggregated Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data is still squashed over on the left side of the plot.\n",
    "# Try a log transformation and plot the transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = ['value', 'freq', 'recent']\n",
    "\n",
    "df_agg_log = df_agg[selected_columns].copy()\n",
    "\n",
    "df_agg_log['value'] = np.log1p(df_agg_log['value'])\n",
    "df_agg_log['freq'] = np.log1p(df_agg_log['freq'])\n",
    "df_agg_log['recent'] = np.log1p(df_agg_log['recent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "ax = fig.add_subplot(projection=\"3d\")                \n",
    "\n",
    "scatter = ax.scatter(df_agg_log['value'], df_agg_log['freq'],\n",
    "                      df_agg_log['recent'])\n",
    "\n",
    "ax.set_xlabel('Deposit Value')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_zlabel('Recency')\n",
    "\n",
    "ax.set_title('3-D Scatterplot of Log Transformed Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This a much better distribution of the data\n",
    "\n",
    "# See how clustering this data looks without further scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First pass to determine optimum number of clusters \n",
    "# to use for KMeans\n",
    "\n",
    "max_k = 12\n",
    "inertia = []\n",
    "silhouette_scores = []\n",
    "k_values = range(2, 12)\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, \n",
    "                    max_iter=1000)\n",
    "    cluster_labels = kmeans.fit_predict(df_agg_log)\n",
    "    sil_score = silhouette_score(df_agg_log, \n",
    "                                 cluster_labels)\n",
    "    silhouette_scores.append(sil_score)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot( 1, 2, 1)\n",
    "plt.plot(k_values, inertia, marker = 'o')\n",
    "plt.title(\"KMeans Inertia for DIfferent Values of k\")\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.xticks(k_values)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot( 1, 2, 2)\n",
    "plt.plot(k_values, silhouette_scores, marker = 'o',\n",
    "          color='orange')\n",
    "plt.title(\"Silhouette Scores for DIfferent Values of k\")\n",
    "plt.xlabel('Silhouette Score')\n",
    "plt.ylabel('Inertia')\n",
    "plt.xticks(k_values)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette score and inertia both suggest that \n",
    "# 5 clusters will work best for this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=5, random_state=42, \n",
    "                max_iter=1000)\n",
    "\n",
    "cluster_labels = kmeans.fit_predict(df_agg_log)\n",
    "\n",
    "cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_log['clusters'] = cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_log.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -D Colour Plot the data to id the Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_colors = {0: '#1f77b4', #Blue\n",
    "                  1: '#ff7f0e', #Orange\n",
    "                  2: '#2ca02c', #Green\n",
    "                  3: '#d62728', #Red\n",
    "                  4: '#800080', #Purple\n",
    "                  }\n",
    "\n",
    "colors = df_agg_log['clusters'].map(cluster_colors)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "ax = fig.add_subplot(projection=\"3d\")                \n",
    "\n",
    "scatter = ax.scatter(df_agg_log['value'],\n",
    "                    df_agg_log['freq'],\n",
    "                    df_agg_log['recent'],\n",
    "                    c=colors,\n",
    "                    marker='o')\n",
    "\n",
    "ax.set_xlabel('Deposit Value')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_zlabel('Recency')\n",
    "\n",
    "ax.set_title('3-D Scatterplot of Log Transformed Data by Cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The scatter plot shows nicely defined clusters.\n",
    "\n",
    "# Proceed to violin plots to refine the visualization of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 18))\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "sns.violinplot(x=df_agg_log['clusters'], y=df_agg_log['value'], \n",
    "               palette=cluster_colors,\n",
    "               hue=df_agg_log['clusters'])\n",
    "\n",
    "sns.violinplot(y=df_agg_log['value'], color='gray', \n",
    "               linewidth=1.0)\n",
    "plt.title('Monetary Value by Cluster')\n",
    "plt.ylabel('Deposit Value')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "\n",
    "sns.violinplot(x=df_agg_log['clusters'], y=df_agg_log['freq'], \n",
    "               palette=cluster_colors,\n",
    "               hue=df_agg_log['clusters'])\n",
    "\n",
    "sns.violinplot(y=df_agg_log['freq'], color='gray', \n",
    "               linewidth=1.0)\n",
    "plt.title('Frequency by Cluster')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "sns.violinplot(x=df_agg_log['clusters'], y=df_agg_log['recent'], \n",
    "               palette=cluster_colors,\n",
    "               hue=df_agg_log['clusters'])\n",
    "\n",
    "sns.violinplot(y=df_agg_log['recent'], color='gray', \n",
    "               linewidth=1.0)\n",
    "plt.title('Recency by Cluster')\n",
    "plt.ylabel('Recency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "g_kmeans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
